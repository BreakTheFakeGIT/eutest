. I am gonna create exper...



Dzisiaj
You said:

I use Langchain framework. I am gonna create experiments with parameters. 
I have got html text. 
I want to split text into chunks using diffrent tags. 
My tags are 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'p', 'b', 'strong'
I want to exclude some words, phrases and regex.
I want to to drop first chunk where is if statament 
I want to add local Ollama model to create embeddings 
I want to calculate cosinsus corelation.
I want to use metadata. I use pgvector
I run local Ollama model to exctract information with diffrent prompts for whole text
For each chunk I run local Ollama model to exctract information with diffrent prompts. 
For each chunck I am gonna usuing agent.
I want to collect that information. 
I use psycopg 3 not alchemy.
In the next step I want to insert into postgres data base. I use psycopg 3 not alchemy.
Finally I want to select data from database
Write a function code in Python.

Copilot said:
Copilot
Thanks for the detailed description! Based on your setup and goals, here's a Python function that integrates the LangChain framework, local Ollama model, HTML chunking, metadata handling, cosine similarity, and PostgreSQL (via psycopg3 and pgvector). This is a modular starting point you can expand for experimentation.

âœ… Assumptions:
You have a local Ollama model running (e.g., llama3, mistral, etc.).
You use LangChain's OllamaEmbeddings and Ollama for embedding and LLM tasks.
You use psycopg3 with pgvector extension enabled.
You want to chunk HTML using specific tags and exclude certain patterns.
ðŸ§  Python Function Overview



Python
import re
from bs4 import BeautifulSoup
from langchain.embeddings import OllamaEmbeddings
from langchain.llms import Ollama
from langchain.vectorstores.pgvector import PGVector
from langchain.docstore.document import Document
from langchain.agents import initialize_agent, Tool
from langchain.agents.agent_types import AgentType
import psycopg

# Configuration
EXCLUDED_PATTERNS = [r'\bexample\b', r'\btest\b', r'\d{4}-\d{2}-\d{2}']  # Add your regex/phrases
CHUNK_TAGS = ['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'p', 'b', 'strong']
PGVECTOR_CONFIG = {
    "connection_string": "postgresql://user:password@localhost:5432/dbname",
    "collection_name": "html_chunks",
    "embedding_function": OllamaEmbeddings(model="llama3")
}

def process_html_and_store(html_text: str):
    soup = BeautifulSoup(html_text, 'html.parser')
    chunks = []

    # Extract chunks
    for tag in CHUNK_TAGS:
        for element in soup.find_all(tag):
            text = element.get_text(strip=True)
            if any(re.search(pattern, text, re.IGNORECASE) for pattern in EXCLUDED_PATTERNS):
                continue
            chunks.append(text)

    # Drop first chunk with 'if' statement
    chunks = [chunk for chunk in chunks if not chunk.strip().lower().startswith('if')]

    # Create LangChain documents with metadata
    documents = [
        Document(page_content=chunk, metadata={"tag": tag})
        for chunk, tag in zip(chunks, [tag for tag in CHUNK_TAGS for _ in soup.find_all(tag)])
    ]

    # Embedding and storing in PGVector
    vectorstore = PGVector(**PGVECTOR_CONFIG)
    vectorstore.add_documents(documents)

    # Cosine similarity example (querying)
    query = "Find relevant chunks about security"
    results = vectorstore.similarity_search(query, k=5)

    # Run Ollama model on full text
    llm = Ollama(model="llama3")
    full_text_info = llm.invoke(f"Extract key insights from the following HTML:\n{html_text}")

    # Run agent on each chunk
    tools = [Tool(name="OllamaExtractor", func=llm.invoke, description="Extract info from chunk")]
    agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION)

    chunk_infos = []
    for doc in documents:
        response = agent.run(f"Extract insights from: {doc.page_content}")
        chunk_infos.append({
            "content": doc.page_content,
            "metadata": doc.metadata,
            "insights": response
        })

    # Insert into PostgreSQL using psycopg3
    with psycopg.connect(PGVECTOR_CONFIG["connection_string"]) as conn:
        with conn.cursor() as cur:
            for info in chunk_infos:
                cur.execute(
                    "INSERT INTO html_chunks (content, metadata, insights) VALUES (%s, %s, %s)",
                    (info["content"], str(info["metadata"]), info["insights"])
                )
            conn.commit()

    # Select data
    with psycopg.connect(PGVECTOR_CONFIG["connection_string"]) as conn:
        with conn.cursor() as cur:
            cur.execute("SELECT content, insights FROM html_chunks LIMIT 10")
            rows = cur.fetchall()
            for row in rows:
                print("Content:", row[0])
                print("Insights:", row[1])
                print("------")






import re
from bs4 import BeautifulSoup
from langchain.embeddings import OllamaEmbeddings
from langchain.llms import Ollama
from langchain.vectorstores.pgvector import PGVector
from langchain.docstore.document import Document
from langchain.agents import initialize_agent, Tool
from langchain.agents.agent_types import AgentType
import psycopg
from typing import List

# Configuration
EXCLUDED_PATTERNS = [r'\bexample\b', r'\btest\b', r'\d{4}-\d{2}-\d{2}']
CHUNK_TAGS = ['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'p', 'b', 'strong']
BATCH_SIZE = 10

PGVECTOR_CONFIG = {
    "connection_string": "postgresql://user:password@localhost:5432/dbname",
    "collection_name": "html_chunks",
    "embedding_function": OllamaEmbeddings(model="llama3")
}

def chunk_html(html_text: str) -> List[Document]:
    soup = BeautifulSoup(html_text, 'html.parser')
    chunks = []

    for tag in CHUNK_TAGS:
        for element in soup.find_all(tag):
            text = element.get_text(strip=True)
            if any(re.search(pattern, text, re.IGNORECASE) for pattern in EXCLUDED_PATTERNS):
                continue
            chunks.append((text, tag))

    # Drop first chunk with 'if' statement
    chunks = [chunk for chunk in chunks if not chunk[0].strip().lower().startswith('if')]

    return [Document(page_content=text, metadata={"tag": tag}) for text, tag in chunks]

def batch_process_documents(documents: List[Document]):
    llm = Ollama(model="llama3")
    vectorstore = PGVector(**PGVECTOR_CONFIG)
    tools = [Tool(name="OllamaExtractor", func=llm.invoke, description="Extract info from chunk")]
    agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION)

    # Batch embedding and storing
    for i in range(0, len(documents), BATCH_SIZE):
        batch = documents[i:i + BATCH_SIZE]
        vectorstore.add_documents(batch)

    # Batch agent processing
    chunk_infos = []
    for i in range(0, len(documents), BATCH_SIZE):
        batch = documents[i:i + BATCH_SIZE]
        for doc in batch:
            response = agent.run(f"Extract insights from: {doc.page_content}")
            chunk_infos.append({
                "content": doc.page_content,
                "metadata": doc.metadata,
                "insights": response
            })

    # Batch insert into PostgreSQL
    with psycopg.connect(PGVECTOR_CONFIG["connection_string"]) as conn:
        with conn.cursor() as cur:
            for i in range(0, len(chunk_infos), BATCH_SIZE):
                batch = chunk_infos[i:i + BATCH_SIZE]
                cur.executemany(
                    "INSERT INTO html_chunks (content, metadata, insights) VALUES (%s, %s, %s)",
                    [(info["content"], str(info["metadata"]), info["insights"]) for info in batch]
                )
            conn.commit()

def process_html_and_store(html_text: str):
    documents = chunk_html(html_text)

    # Full text processing
    llm = Ollama(model="llama3")
    full_text_info = llm.invoke(f"Extract key insights from the following HTML:\n{html_text}")
    print("Full Text Insights:", full_text_info)

    # Batch processing
    batch_process_documents(documents)

    # Select and print results
    with psycopg.connect(PGVECTOR_CONFIG["connection_string"]) as conn:
        with conn.cursor() as cur:
            cur.execute("SELECT content, insights FROM html_chunks LIMIT 10")
            rows = cur.fetchall()
            for row in rows:
                print("Content:", row[0])
                print("Insights:", row[1])









âœ… Key Enhancements:
Uses asyncio for concurrency.
Uses run_in_executor to parallelize blocking LLM calls.
Keeps batch processing logic intact.
Ensures PostgreSQL insertion is still done in batches.
ðŸ§  Updated Python Code with Async Parallel Processing



Python
import re
import asyncio
from bs4 import BeautifulSoup
from langchain.embeddings import OllamaEmbeddings
from langchain.llms import Ollama
from langchain.vectorstores.pgvector import PGVector
from langchain.docstore.document import Document
from langchain.agents import initialize_agent, Tool
from langchain.agents.agent_types import AgentType
import psycopg
from typing import List

# Configuration
EXCLUDED_PATTERNS = [r'\bexample\b', r'\btest\b', r'\d{4}-\d{2}-\d{2}']
CHUNK_TAGS = ['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'p', 'b', 'strong']
BATCH_SIZE = 10

PGVECTOR_CONFIG = {
    "connection_string": "postgresql://user:password@localhost:5432/dbname",
    "collection_name": "html_chunks",
    "embedding_function": OllamaEmbeddings(model="llama3")
}

def chunk_html(html_text: str) -> List[Document]:
    soup = BeautifulSoup(html_text, 'html.parser')
    chunks = []

    for tag in CHUNK_TAGS:
        for element in soup.find_all(tag):
            text = element.get_text(strip=True)
            if any(re.search(pattern, text, re.IGNORECASE) for pattern in EXCLUDED_PATTERNS):
                continue
            chunks.append((text, tag))

    # Drop first chunk with 'if' statement
    chunks = [chunk for chunk in chunks if not chunk[0].strip().lower().startswith('if')]

    return [Document(page_content=text, metadata={"tag": tag}) for text, tag in chunks]

async def extract_insight_async(agent, doc: Document):
    loop = asyncio.get_event_loop()
    return await loop.run_in_executor(None, agent.run, f"Extract insights from: {doc.page_content}")

async def batch_process_documents_async(documents: List[Document]):
    llm = Ollama(model="llama3")
    vectorstore = PGVector(**PGVECTOR_CONFIG)
    tools = [Tool(name="OllamaExtractor", func=llm.invoke, description="Extract info from chunk")]
    agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION)

    # Batch embedding and storing
    for i in range(0, len(documents), BATCH_SIZE):
        batch = documents[i:i + BATCH_SIZE]
        vectorstore.add_documents(batch)

    # Async agent processing
    chunk_infos = []
    for i in range(0, len(documents), BATCH_SIZE):
        batch = documents[i:i + BATCH_SIZE]
        tasks = [extract_insight_async(agent, doc) for doc in batch]
        responses = await asyncio.gather(*tasks)

        for doc, response in zip(batch, responses):
            chunk_infos.append({
                "content": doc.page_content,
                "metadata": doc.metadata,
                "insights": response
            })

    # Batch insert into PostgreSQL
    with psycopg.connect(PGVECTOR_CONFIG["connection_string"]) as conn:
        with conn.cursor() as cur:
            for i in range(0, len(chunk_infos), BATCH_SIZE):
                batch = chunk_infos[i:i + BATCH_SIZE]
                cur.executemany(
                    "INSERT INTO html_chunks (content, metadata, insights) VALUES (%s, %s, %s)",
                    [(info["content"], str(info["metadata"]), info["insights"]) for info in batch]
                )
            conn.commit()

async def process_html_and_store_async(html_text: str):
    documents = chunk_html(html_text)

    # Full text processing
    llm = Ollama(model="llama3")
    loop = asyncio.get_event_loop()
    full_text_info = await loop.run_in_executor(None, llm.invoke, f"Extract key insights from the following HTML:\n{html_text}")
    print("Full Text Insights:", full_text_info)

    # Async batch processing
    await batch_process_documents_async(documents)

    # Select and print results
    with psycopg.connect(PGVECTOR_CONFIG["connection_string"]) as conn:
        with conn.cursor() as cur:
            cur.execute("SELECT content, insights FROM html_chunks LIMIT 10")
            rows = cur.fetchall()
            for row in rows:
                print("Content:", row[0])
                print("Insights:", row[1])
                print("------")

# To run the async function
# asyncio.run(process_html_and_store_async(html_text))


PokaÅ¼ mniej
ðŸ§ª How to Use:
Call the final function like this:




Python
import asyncio

html_text = "<html>...</html>"  # Your HTML input
asyncio.run(process_html_and_store_async(html_text))
Would you like me to:

Help you create the PostgreSQL table schema?
Add logging or error handling?
Add support for multiple prompt templates per chunk?
Let me know how you'd like to evolve this pipeline!




PROMPT_TEMPLATES = [
    "Summarize the following content:\n{content}",
    "Extract key entities from the following content:\n{content}",
    "Classify the topic of the following content:\n{content}"
]


async def extract_insight_async(agent, doc: Document):
    loop = asyncio.get_event_loop()
    tasks = [
        loop.run_in_executor(None, agent.run, prompt.format(content=doc.page_content))
        for prompt in PROMPT_TEMPLATES
    ]
    return await asyncio.gather(*tasks)


async def batch_process_documents_async(documents: List[Document]):
    llm = Ollama(model="llama3")
    vectorstore = PGVector(**PGVECTOR_CONFIG)
    tools = [Tool(name="OllamaExtractor", func=llm.invoke, description="Extract info from chunk")]
    agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION)

    # Batch embedding and storing
    for i in range(0, len(documents), BATCH_SIZE):
        batch = documents[i:i + BATCH_SIZE]
        vectorstore.add_documents(batch)

    # Async agent processing
    chunk_infos = []
    for i in range(0, len(documents), BATCH_SIZE):
        batch = documents[i:i + BATCH_SIZE]
        tasks = [extract_insight_async(agent, doc) for doc in batch]
        responses = await asyncio.gather(*tasks)

        for doc, insights in zip(batch, responses):
            chunk_infos.append({
                "content": doc.page_content,
                "metadata": doc.metadata,
                "insights": insights  # List of responses per prompt
            })

    # Batch insert into PostgreSQL
    with psycopg.connect(PGVECTOR_CONFIG["connection_string"]) as conn:
        with conn.cursor() as cur:
            for i in range(0, len(chunk_infos), BATCH_SIZE):
                batch = chunk_infos[i:i + BATCH_SIZE]
                cur.executemany(
                    "INSERT INTO html_chunks (content, metadata, insights) VALUES (%s, %s, %s)",
                    [(info["content"], str(info["metadata"]), str(info["insights"])) for info in batch]
                )
            conn.commit()





[
  "Summary: ...",
  "Entities: ...",
  "Topic: ..."
]


1. html_chunks
Stores the base chunk and metadata.


SQL
CREATE TABLE html_chunks (
    id SERIAL PRIMARY KEY,
    content TEXT NOT NULL,
    metadata JSONB,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

2. chunk_prompts
Stores each prompt and its response, linked to html_chunks.

SQL
CREATE TABLE chunk_prompts (
    id SERIAL PRIMARY KEY,
    chunk_id INTEGER REFERENCES html_chunks(id) ON DELETE CASCADE,
    prompt TEXT NOT NULL,
    response TEXT NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);


SQL
SELECT c.content, p.prompt, p.response
FROM html_chunks c
JOIN chunk_prompts p ON c.id = p.chunk_id
WHERE c.id = 42;



Python
import psycopg
from typing import List, Dict

def insert_chunk_with_prompts(
    connection_string: str,
    content: str,
    metadata: Dict,
    prompt_responses: List[Dict[str, str]]
):
    """
    Inserts a chunk and its associated prompt responses into PostgreSQL.

    Parameters:
    - connection_string: PostgreSQL connection string
    - content: HTML chunk text
    - metadata: dictionary with metadata (e.g., tag info)
    - prompt_responses: list of dicts with 'prompt' and 'response' keys
    """
    with psycopg.connect(connection_string) as conn:
        with conn.cursor() as cur:
            # Insert into html_chunks
            cur.execute(
                "INSERT INTO html_chunks (content, metadata) VALUES (%s, %s) RETURNING id",
                (content, metadata)
            )
            chunk_id = cur.fetchone()[0]

            # Insert each prompt-response pair
            cur.executemany(
                "INSERT INTO chunk_prompts (chunk_id, prompt, response) VALUES (%s, %s, %s)",
                [(chunk_id, pr["prompt"], pr["response"]) for pr in prompt_responses]
            )

        conn.commit()



Python
html_chunk = "<p>This is a sample paragraph about cybersecurity.</p>"
metadata = {"tag": "p"}

prompt_responses = [
    {"prompt": "Summarize the following content:\n{content}", "response": "This paragraph discusses cybersecurity."},
    {"prompt": "Extract key entities from the following content:\n{content}", "response": "Entities: cybersecurity"},
    {"prompt": "Classify the topic of the following content:\n{content}", "response": "Topic: Technology"}
]

insert_chunk_with_prompts(
    connection_string="postgresql://user:password@localhost:5432/dbname",
    content=html_chunk,
    metadata=metadata,
    prompt_responses=prompt_responses
)



Python
import psycopg
from typing import Optional, List, Dict

def retrieve_chunk_insights(
    connection_string: str,
    chunk_id: Optional[int] = None,
    tag: Optional[str] = None,
    prompt_filter: Optional[str] = None,
    limit: int = 10
) -> List[Dict]:
    """
    Retrieves insights from the database with optional filters.

    Parameters:
    - connection_string: PostgreSQL connection string
    - chunk_id: filter by specific chunk ID
    - tag: filter by HTML tag in metadata
    - prompt_filter: filter by prompt text (partial match)
    - limit: max number of results to return

    Returns:
    - List of dictionaries with content, prompt, response, and metadata
    """
    query = """
        SELECT c.id, c.content, c.metadata, p.prompt, p.response
        FROM html_chunks c
        JOIN chunk_prompts p ON c.id = p.chunk_id
        WHERE TRUE
    """
    params = []

    if chunk_id is not None:
        query += " AND c.id = %s"
        params.append(chunk_id)

    if tag is not None:
        query += " AND c.metadata ->> 'tag' = %s"
        params.append(tag)

    if prompt_filter is not None:
        query += " AND p.prompt ILIKE %s"
        params.append(f"%{prompt_filter}%")

    query += " ORDER BY c.created_at DESC LIMIT %s"
    params.append(limit)

    results = []
    with psycopg.connect(connection_string) as conn:
        with conn.cursor() as cur:
            cur.execute(query, params)
            rows = cur.fetchall()
            for row in rows:
                results.append({
                    "chunk_id": row[0],
                    "content": row[1],
                    "metadata": row[2],
                    "prompt": row[3],
                    "response": row[4]
                })



PokaÅ¼ wiÄ™cej wierszy
âœ… Example Usage



Python
connection_string = "postgresql://user:password@localhost:5432/dbname"

# Get all insights for chunks with tag 'p'
results = retrieve_chunk_insights(connection_string, tag='p')

for r in results:
    print(f"Chunk ID: {r['chunk_id']}")
    print(f"Tag: {r['metadata'].get('tag')}")
    print(f"Prompt: {r['prompt']}")
    print(f"Response: {r['response']}")
    print("------")
Would you like to:

Add export to CSV or JSON?
Add support for full-text search in responses?
Visualize insights with charts or graphs?
Let me know how you'd like to build on this!








You said:

Support full-text search in responses
Copilot said:
Copilot
To support full-text search in responses from the chunk_prompts table, weâ€™ll do two things:

âœ… 1. Update PostgreSQL Schema for Full-Text Search
Add a tsvector column and an index to enable fast full-text search:




SQL
-- Add a tsvector column for full-text search
ALTER TABLE chunk_prompts
ADD COLUMN response_tsv tsvector GENERATED ALWAYS AS (to_tsvector('english', response)) STORED;

-- Create a GIN index on the tsvector column
CREATE INDEX idx_chunk_prompts_response_tsv ON chunk_prompts USING GIN (response_tsv);
This allows efficient full-text queries using @@ operator.

âœ… 2. Python Retrieval Function with Full-Text Search



Python
def search_responses_full_text(
    connection_string: str,
    search_query: str,
    limit: int = 10
) -> List[Dict]:
    """
    Performs full-text search on responses in chunk_prompts.

    Parameters:
    - connection_string: PostgreSQL connection string
    - search_query: text to search for
    - limit: max number of results

    Returns:
    - List of dictionaries with chunk content, prompt, response, and metadata
    """
    query = """
        SELECT c.id, c.content, c.metadata, p.prompt, p.response
        FROM html_chunks c
        JOIN chunk_prompts p ON c.id = p.chunk_id
        WHERE p.response_tsv @@ plainto_tsquery(%s)
        ORDER BY p.created_at DESC
        LIMIT %s
    """

    results = []
    with psycopg.connect(connection_string) as conn:
        with conn.cursor() as cur:
            cur.execute(query, (search_query, limit))
            rows = cur.fetchall()
            for row in rows:
                results.append({
                    "chunk_id": row[0],
                    "content": row[1],
                    "metadata": row[2],
                    "prompt": row[3],
                    "response": row[4]
                })

    return results


PokaÅ¼ wiÄ™cej wierszy
ðŸ§ª Example Usage



Python
results = search_responses_full_text(
    connection_string="postgresql://user:password@localhost:5432/dbname",
    search_query="cybersecurity"
)

for r in results:
    print(f"Chunk ID: {r['chunk_id']}")
    print(f"Prompt: {r['prompt']}")
    print(f"Response: {r['response']}")
    print("------")
Would you like to:

Add support for ranking results by relevance?
Combine full-text search with tag or prompt filters?
Export search results to a file?
Let me know how you'd like to extend this!








You said:

Add ranking by relevance
Copilot said:
Copilot
To add ranking by relevance to your full-text search in PostgreSQL, weâ€™ll use the ts_rank function, which scores how well each document matches the query.

âœ… Updated SQL Query with Relevance Ranking



SQL
SELECT 
    c.id, 
    c.content, 
    c.metadata, 
    p.prompt, 
    p.response,
    ts_rank(p.response_tsv, plainto_tsquery(%s)) AS rank
FROM html_chunks c
JOIN chunk_prompts p ON c.id = p.chunk_id
WHERE p.response_tsv @@ plainto_tsquery(%s)
ORDER BY rank DESC
LIMIT %s;
ðŸ§© Updated Python Function



Python
def search_responses_ranked(
    connection_string: str,
    search_query: str,
    limit: int = 10
) -> List[Dict]:
    """
    Performs full-text search on responses and ranks results by relevance.

    Parameters:
    - connection_string: PostgreSQL connection string
    - search_query: text to search for
    - limit: max number of results

    Returns:
    - List of dictionaries with chunk content, prompt, response, metadata, and rank
    """
    query = """
        SELECT c.id, c.content, c.metadata, p.prompt, p.response,
               ts_rank(p.response_tsv, plainto_tsquery(%s)) AS rank
        FROM html_chunks c
        JOIN chunk_prompts p ON c.id = p.chunk_id
        WHERE p.response_tsv @@ plainto_tsquery(%s)
        ORDER BY rank DESC
        LIMIT %s
    """

    results = []
    with psycopg.connect(connection_string) as conn:
        with conn.cursor() as cur:
            cur.execute(query, (search_query, search_query, limit))
            rows = cur.fetchall()
            for row in rows:
                results.append({
                    "chunk_id": row[0],
                    "content": row[1],
                    "metadata": row[2],
                    "prompt": row[3],
                    "response": row[4],
                    "rank": row[5]
                })

    return results



results = search_responses_ranked(
    connection_string="postgresql://user:password@localhost:5432/dbname",
    search_query="cybersecurity"
)

for r in results:
    print(f"Rank: {r['rank']:.2f}")
    print(f"Chunk ID: {r['chunk_id']}")
    print(f"Prompt: {r['prompt']}")
    print(f"Response: {r['response']}")
    print("------")

