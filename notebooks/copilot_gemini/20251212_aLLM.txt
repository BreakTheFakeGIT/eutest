import asyncio
import time
from typing import List, Dict, Any, Tuple
import json
import os

import psycopg
from psycopg.rows import dict_row
from psycopg_pool import AsyncConnectionPool

from sentence_transformers import SentenceTransformer
from langchain_ollama import ChatOllama
from langchain_core.prompts import ChatPromptTemplate
from langchain_text_splitters import RecursiveCharacterTextSplitter

# -----------------------------
# Configuration
# -----------------------------
PG_DSN = 'postgresql://krzysiek@localhost:5432/eureka' #os.getenv("PG_DSN", "postgresql://user:password@localhost:5432/yourdb")
POOL_MIN = 1
POOL_MAX = 10

# Choose a strong multilingual embedding for Polish
EMBED_MODEL_NAME = '/c/Users/Krzysiek/ai_tools/models/all-MiniLM-L6-v2'
EMBED_DIM = 768

# LLM models available in Ollama (ensure they are pulled locally)
LLM_MODELS = ['hf.co/pswitala/pllum-8b-instruct-q5_k_m_gguf:Q5_K_M']  # rename to your local tags if needed

# Concurrency controls
MAX_PARALLEL_LLM_CALLS = 6   # tune per machine
MAX_PARALLEL_DB_WRITES = 10

# -----------------------------
# Helpers
# -----------------------------
def to_pgvector_literal(vec: List[float]) -> str:
    """Convert a list of floats to pgvector textual literal: '[0.1,0.2,...]'"""
    return "[" + ",".join(f"{x:.8f}" for x in vec) + "]"

def now_ms() -> int:
    return int(time.perf_counter() * 1000)

# -----------------------------
# Async DB pool
# -----------------------------
pool = AsyncConnectionPool(
    conninfo=PG_DSN,
    min_size=POOL_MIN,
    max_size=POOL_MAX,
    open=True,  # open immediately
)

# -----------------------------
# Tax-type question bank (10 per type)
# -----------------------------
def questions_for_tax_type(tax_type: str) -> List[str]:
    # Customize per your domain; ensuring 10 questions per context
    base = tax_type.upper()
    return [
        f"{base}: Jakie są podstawowe obowiązki podatnika?",
        f"{base}: Jakie stawki podatkowe mają zastosowanie?",
        f"{base}: Jakie zwolnienia lub ulgi mogą mieć zastosowanie?",
        f"{base}: Jak wygląda procedura rozliczeniowa?",
        f"{base}: Jakie dokumenty są wymagane?",
        f"{base}: Jakie są typowe ryzyka i błędy?",
        f"{base}: Przykłady zastosowania w praktyce.",
        f"{base}: Warunki i ograniczenia.",
        f"{base}: Terminy i sankcje.",
        f"{base}: Odniesienia do przepisów (artykuły, paragrafy).",
    ]

# -----------------------------
# Prompt template for LLMs
# -----------------------------
QA_PROMPT = ChatPromptTemplate.from_messages([
    ("system", "Jesteś ekspertem podatkowym. Odpowiadaj krótko, rzeczowo i po polsku."),
    ("user",
     "Kontekst (fragment):\n{context}\n\n"
     "Pytanie:\n{question}\n\n"
     "Podaj odpowiedź w 1-3 zdaniach, zwięźle. Jeśli brak danych, powiedz: 'Brak wystarczającego kontekstu.'"
    ),
])

def build_llms() -> Dict[str, ChatOllama]:
    """Return LLM clients for Ollama models; synchronous clients wrapped later."""
    return {name: ChatOllama(model=name, temperature=0.0) for name in LLM_MODELS}

# -----------------------------
# Chunking
# -----------------------------
def chunk_text(text: str,
               chunk_size: int = 1200,
               chunk_overlap: int = 120) -> List[str]:
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        length_function=len,
        #separators=["\n\n", "\n", ".", " ", ""],
    )
    return splitter.split_text(text)

# -----------------------------
# Data access
# -----------------------------
async def fetch_texts_batch(limit: int = 10, tax_type: str | None = None) -> List[Dict[str, Any]]:
    sql = "SELECT id, tax_type, source_uri, body FROM tax_texts"
    params: Tuple[Any, ...] = ()
    if tax_type:
        sql += " WHERE tax_type = %s"
        params = (tax_type,)
    sql += " ORDER BY id DESC LIMIT %s"
    params = params + (limit,)

    async with pool.connection() as conn:
        async with conn.cursor(row_factory=dict_row) as cur:
            await cur.execute(sql, params)
            rows = await cur.fetchall()
            return rows

async def insert_qa_result(conn: psycopg.AsyncConnection,
                           text_id: int, chunk_id: int, question: str,
                           model_name: str, answer: str, latency_ms: int) -> int:
    sql = """
    INSERT INTO qa_results (text_id, chunk_id, question, model_name, answer, latency_ms)
    VALUES (%s, %s, %s, %s, %s, %s)
    RETURNING id
    """
    async with conn.cursor() as cur:
        await cur.execute(sql, (text_id, chunk_id, question, model_name, answer, latency_ms))
        new_id = (await cur.fetchone())[0]
        return new_id

async def insert_answer_embedding(conn: psycopg.AsyncConnection,
                                  qa_result_id: int, tax_type: str, vec: List[float]) -> None:
    sql = """
    INSERT INTO answer_embeddings (qa_result_id, tax_type, embedding)
    VALUES (%s, %s, %s::vector)
    """
    literal = to_pgvector_literal(vec)
    async with conn.cursor() as cur:
        await cur.execute(sql, (qa_result_id, tax_type, literal))

async def insert_answer_embedding(conn: psycopg.AsyncConnection,
                                  qa_result_id: int, tax_type: str, vec: List[float]) -> None:
    sql = """
    INSERT INTO answer_embeddings (qa_result_id, tax_type, embedding)
    VALUES (%s, %s, %s::vector)
    """
    literal = to_pgvector_literal(vec)
    async with conn.cursor() as cur:
        await cur.execute(sql, (qa_result_id, tax_type, literal))



# -----------------------------
# RAG retrieval (select data)
# -----------------------------
class Retriever:
    def __init__(self, embedder: SentenceTransformer):
        self.embedder = embedder

    async def embed_text(self, text: str) -> List[float]:
        # SentenceTransformer is sync; run off the event loop
        return await asyncio.to_thread(lambda: self.embedder.encode(text, normalize_embeddings=True).tolist())

    async def search_answers(self, query: str, top_k: int = 10,
                             tax_type: str | None = None) -> List[Dict[str, Any]]:
        query_vec = await self.embed_text(query)
        literal = to_pgvector_literal(query_vec)

        where = "WHERE true"
        params: Tuple[Any, ...] = ()
        if tax_type:
            where += " AND tax_type = %s"
            params = (tax_type,)

        sql = f"""
        SELECT qa.id AS qa_result_id,
               qa.text_id,
               qa.chunk_id,
               qa.question,
               qa.model_name,
               qa.answer,
               ae.tax_type,
               ae.embedding <-> %s::vector AS distance
        FROM qa_results qa
        JOIN answer_embeddings ae ON ae.qa_result_id = qa.id
        {where}
        ORDER BY ae.embedding <-> %s::vector
        LIMIT %s
        """
        # we pass the vector twice due to ORDER BY
        params = params + (literal, literal, top_k)

        async with pool.connection() as conn:
            async with conn.cursor(row_factory=dict_row) as cur:
                await cur.execute(sql, params)
                return await cur.fetchall()

# -----------------------------
# Core async processing
# -----------------------------
class TaxRAGPipeline:
    def __init__(self):
        self.llms = build_llms()
        self.embedder = SentenceTransformer(EMBED_MODEL_NAME)
        self.retriever = Retriever(self.embedder)
        self.llm_sem = asyncio.Semaphore(MAX_PARALLEL_LLM_CALLS)

    async def llm_answer(self, model_name: str, context: str, question: str) -> Tuple[str, int]:
        """Call LLM synchronously in a thread; measure latency (ms)."""
        prompt = QA_PROMPT.format(context=context, question=question)
        start = now_ms()

        def _invoke():
            # ChatOllama has .invoke(); returns ChatMessage or string depending on version
            res = self.llms[model_name].invoke(prompt)
            # Normalize to string:
            return getattr(res, "content", str(res))

        answer = await asyncio.to_thread(_invoke)
        latency = now_ms() - start
        return answer.strip(), latency

    async def embed_answer(self, text: str) -> List[float]:
        return await asyncio.to_thread(lambda: self.embedder.encode(text, normalize_embeddings=True).tolist())

    async def process_context(self, context_row: Dict[str, Any]) -> Dict[str, Any]:
        """
        Process a single raw text:
          - assign 10 questions by tax_type
          - chunk text
          - for each chunk × question × model: ask, embed, save
          - measure timings
        """
        timings: Dict[str, float] = {}
        t0 = time.perf_counter()

        text_id = context_row["id"]
        tax_type = context_row["tax_type"]
        body = context_row["body"]
        questions = questions_for_tax_type(tax_type)

        # 3) Chunking
        t_chunk_start = time.perf_counter()
        chunks = chunk_text(body)
        timings["chunking_s"] = time.perf_counter() - t_chunk_start

        results_count = 0

        # Pre-open a DB connection for this context to amortize
        async with pool.connection() as conn:
            # 4) For each chunk and each question, query both LLMs
            tasks = []
            for chunk_id, chunk_text_ in enumerate(chunks):
                for q in questions:
                    for model_name in LLM_MODELS:
                        # Limit parallelism
                        async def one_call(chunk_id=chunk_id, chunk_text_=chunk_text_, q=q, model_name=model_name):
                            async with self.llm_sem:
                                answer, latency_ms = await self.llm_answer(model_name, chunk_text_, q)
                            # 6a) Save QA result
                            qa_id = await insert_qa_result(conn, text_id, chunk_id, q, model_name, answer, latency_ms)
                            # 4b/6b) Vectorize answer + save embedding
                            vec = await self.embed_answer(answer)
                            await insert_answer_embedding(conn, qa_id, tax_type, vec)
                            return 1

                        tasks.append(one_call())

            # 5) Measure total LLM phase
            t_llm_start = time.perf_counter()
            # Write in bursts to reduce contention
            done = await asyncio.gather(*tasks)
            timings["qa_llm_s"] = time.perf_counter() - t_llm_start
            results_count = sum(done)

        timings["total_s"] = time.perf_counter() - t0
        return {
            "text_id": text_id,
            "tax_type": tax_type,
            "chunks": len(chunks),
            "questions_per_context": len(questions),
            "models": LLM_MODELS,
            "qa_results_saved": results_count,
            "timings": timings,
        }

    # 8) RAG: public retrieval helper
    async def retrieve(self, query: str, top_k: int = 10, tax_type: str | None = None) -> List[Dict[str, Any]]:
        return await self.retriever.search_answers(query, top_k=top_k, tax_type=tax_type)

# -----------------------------
# Batch orchestrator (1, 7)
# -----------------------------
async def process_batch(limit: int = 5, tax_type: str | None = None) -> Dict[str, Any]:
    """
    1) Download batch of texts from DB
    7) For each context, we have 10 questions (questions_for_tax_type)
    """
    pipeline = TaxRAGPipeline()
    t_fetch_start = time.perf_counter()
    rows = await fetch_texts_batch(limit=limit, tax_type=tax_type)
    t_fetch = time.perf_counter() - t_fetch_start

    if not rows:
        return {"fetched": 0, "timings": {"fetch_s": t_fetch}, "contexts": []}

    # Process contexts in parallel
    contexts_tasks = [pipeline.process_context(r) for r in rows]
    t_proc_start = time.perf_counter()
    contexts_results = await asyncio.gather(*contexts_tasks)
    t_proc = time.perf_counter() - t_proc_start

    return {
        "fetched": len(rows),
        "timings": {"fetch_s": t_fetch, "process_s": t_proc},
        "contexts": contexts_results,
    }

# -----------------------------
# Example usage
# -----------------------------
if __name__ == "__main__":
    async def main():
        summary = await process_batch(limit=3, tax_type="VAT")
        print(json.dumps(summary, ensure_ascii=False, indent=2))

        # RAG query example:
        pipeline = TaxRAGPipeline()
        hits = await pipeline.retrieve("Stawki VAT dla usług elektronicznych", top_k=5, tax_type="VAT")
        print("\nTop-5 RAG hits:")
        for h in hits:
            print(f"- QA#{h['qa_result_id']} (model={h['model_name']}, dist={h['distance']:.4f})")
            print(f"  Q: {h['question']}\n  A: {h['answer'][:200]}...\n")

    asyncio.run(main())
    
    

# -- Run once
# CREATE EXTENSION IF NOT EXISTS vector;

# -- Raw texts to process
# CREATE TABLE IF NOT EXISTS tax_texts (
#   id BIGSERIAL PRIMARY KEY,
#   tax_type TEXT NOT NULL,              -- e.g., 'VAT', 'CIT', 'PIT'
#   source_uri TEXT,                     -- optional
#   body TEXT NOT NULL,
#   created_at TIMESTAMPTZ DEFAULT NOW()
# );

# -- QA results per chunk/question/model
# CREATE TABLE IF NOT EXISTS qa_results (
#   id BIGSERIAL PRIMARY KEY,
#   text_id BIGINT NOT NULL REFERENCES tax_texts(id),
#   chunk_id INTEGER NOT NULL,
#   question TEXT NOT NULL,
#   model_name TEXT NOT NULL,            -- 'bielik' | 'plumm'
#   answer TEXT NOT NULL,
#   tokens_used INTEGER,                 -- optional
#   latency_ms INTEGER NOT NULL,
#   created_at TIMESTAMPTZ DEFAULT NOW()
# );

# -- Embeddings of answers for retrieval/RAG
# CREATE TABLE IF NOT EXISTS answer_embeddings (
#   id BIGSERIAL PRIMARY KEY,
#   qa_result_id BIGINT NOT NULL REFERENCES qa_results(id) ON DELETE CASCADE,
#   tax_type TEXT NOT NULL,
#   embedding VECTOR(768) NOT NULL,      -- using paraphrase-multilingual-mpnet-base-v2
#   created_at TIMESTAMPTZ DEFAULT NOW()
# );

# -- Index for fast similarity search (cosine distance)
# CREATE INDEX IF NOT EXISTS idx_answer_embeddings_cosine
#   ON answer_embeddings USING ivfflat (embedding vector_cosine_ops) WITH (lists = 100);
# ANALYZE answer_embeddings;

####################################

def split_range_into_intervals(min_val: int, max_val: int, interval_size: int, reverse_order: bool = False) -> list[tuple[int, int]]:
    """
    Splits an integer range [min_val, max_val] into a list of fixed-size
    intervals (tuples of begin and end).
    """
    if min_val >= max_val:
        raise ValueError("min_val must be strictly less than max_val.")
    if interval_size <= 0:
        raise ValueError("interval_size must be a positive integer.")

    intervals = []
    current_start = min_val

    # Iterate through the range, creating intervals
    while current_start <= max_val:
        current_end = current_start + interval_size

        if current_end > max_val + 1:
            current_end = max_val + 1
        
        if current_start < max_val + 1:
            intervals.append((current_start, current_end))
        
        current_start = current_end
    
    # Reverse the order if requested (High to Low)
    if reverse_order:
        intervals.reverse()
        
    return intervals


if __name__ == "__main__":

    # --- Examples ---

    # Example 1: Perfect split
    min_val_1, max_val_1, size_1 = 0, 9, 5
    result_1 = split_range_into_intervals(min_val_1, max_val_1, size_1)

    # Example 2: Imperfect split (remainder)
    min_val_2, max_val_2, size_2 = 10, 22, 5
    result_2 = split_range_into_intervals(min_val_2, max_val_2, size_2)

    # Example 3: Single interval
    min_val_3, max_val_3, size_3 = 50, 52, 10
    result_3 = split_range_into_intervals(min_val_3, max_val_3, size_3, reverse_order=True)

    print(f"--- Example 1: min=0, max=9, size=5 ---")
    print(f"Input Range: [0, 9] (10 total integers)")
    print(f"Output: {result_1}")
    # Expected output: [(0, 5), (5, 10)]

    print(f"\n--- Example 2: min=10, max=22, size=5 ---")
    print(f"Input Range: [10, 22] (13 total integers)")
    print(f"Output: {result_2}")
    # Expected output: [(10, 15), (15, 20), (20, 23)]

    print(f"\n--- Example 3: min=50, max=52, size=10 ---")
    print(f"Input Range: [50, 52] (3 total integers)")
    print(f"Output: {result_3}")
    # Expected output: [(50, 53)]

################################################

import re
from typing import List

def split_ollama_list_output(text: str) -> List[str]:
    """
    Splits a string into a list of items, assuming the items are separated 
    by common list prefixes like '1.', '2.', '-', or '*'.

    Args:
        text: The string output from the model.

    Returns:
        A list of strings, where each element is an item from the list.
    """
    if not text:
        return []

    # --- Regular Expression Explanation ---
    # The pattern looks for the following list separators:
    # 1. Start of string (^) or one or more newlines (\n+)
    # 2. Followed by:
    #    - (\d+\. ): One or more digits followed by a period and a space (e.g., "1. ")
    #    - OR (\|- ): A hyphen followed by a space (e.g., "- ")
    #    - OR (\* ): An asterisk/bullet followed by a space (e.g., "* ")
    # We use re.split() with a capturing group () to keep the delimiter 
    # (e.g., "1. ") as part of the split result, so we can re-attach it later.
    
    # We escape the period '.' as it's a special regex character.
    # The 'r' prefix denotes a raw string, which is good practice for regex.
    pattern = r'(\n+|^)(\d+\.\s+|-{1}\s*|\*{1}\s*)'
    
    # Split the text. The capturing groups in the pattern mean the delimiters 
    # will be included in the resulting list.
    parts = re.split(pattern, text)
    
    # The result of re.split with capturing groups looks like:
    # [leading_text, newline_group1, delimiter_group1, item1_text, 
    #  newline_group2, delimiter_group2, item2_text, ...]
    
    # We need to filter out empty strings and re-combine the delimiter 
    # with its corresponding item text.
    
    final_list = []
    # parts[1:] because parts[0] is often the text *before* the first delimiter, 
    # which we discard unless it's the only text.
    for i in range(2, len(parts), 3):
        # parts[i-1] is the delimiter (e.g., "1. ")
        # parts[i] is the item's text (e.g., "This is the first item")
        if parts[i].strip():
            # Re-attach the delimiter and strip leading/trailing whitespace
            item = (parts[i-1].strip() + parts[i]).strip()
            # Remove any leading newlines that might have been part of the split
            item = re.sub(r'^\n+', '', item)
            final_list.append(item)
    
    # Handle the case where the text has no list formatting but is just a single block of text
    if not final_list and text.strip():
        # If no list items were found, just return the whole text as a single item
        return [text.strip()]

    return final_list

if __name__ == "__main__":

    # --- Example Usage ---
    ollama_output = """
Here are the four key steps:
1. Initialize the model with configuration parameters.
2. Load the dataset for fine-tuning.
3. Begin the training loop, updating weights.
4. Save the final model checkpoint to disk.

- This is a bonus bullet point.
* Another bullet for good measure.
"""

    print("--- Original Text ---")
    print(ollama_output)

    # Split the text
    list_of_steps = split_ollama_list_output(ollama_output)
    print(list_of_steps)
    print("\n--- Processed List ---")
    for i, item in enumerate(list_of_steps):
        print(f"[{i+1}] {item}")

    # --- Edge Case Example ---
    edge_case = "1. First point. 2. Second point (with nested 3. number). 10. Tenth point."
    edge_list = split_ollama_list_output(edge_case)
    print("\n--- Edge Case List ---")
    for i, item in enumerate(edge_list):
        print(f"[{i+1}] {item}")


#########################################################


            # Zaktualizujmy czas zapisu do bazy w osobnym zapytaniu
            # (Prostsze niż próba wstawienia go w pierwszym INSERT)
            if new_id:
                cur.execute(
                    "UPDATE tax_analysis SET db_save_time_ms = %s WHERE id = %s",
                    (timings["db_save_time_ms"], new_id)
                )
                conn.commit()
            
            logging.info(f"Zapisano do bazy (ID: {new_id}) w {timings['db_save_time_ms']:.2f} ms.")

    except (psycopg.Error, Exception) as e:
        logging.error(f"Błąd podczas operacji na bazie danych: {e}")
        conn.rollback()  # Wycofaj zmiany w razie błędu
        return None



