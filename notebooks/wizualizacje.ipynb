{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23c60a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from langchain.schema import Document\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import spacy\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "nlp = spacy.load(\"pl_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab644d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tekst = \"\"\"\n",
    "Interpretacja indywidualna\n",
    "– stanowisko nieprawidłowe\n",
    "Szanowni Państwo,\n",
    "stwierdzam, że Państwa stanowisko w sprawie oceny skutków podatkowych opisanego zdarzenia przyszłego w podatku od towarów i usług jest nieprawidłowe.\n",
    "Zakres wniosku o wydanie interpretacji indywidualnej\n",
    "28 lipca 2025 r. wpłynął Państwa wniosek z 28 lipca 2025 r. o wydanie interpretacji indywidualnej dotyczący opodatkowania paczek wydanych w ramach kampanii reklamowej. Uzupełnili go Państwo – w odpowiedzi na wezwanie – pismem z 4 września 2025 r. (wpływ 4 września 2025 r.).\n",
    "Treść wniosku jest następująca:\n",
    "Opis zdarzenia przyszłego\n",
    "Wnioskodawca w swojej działalności prowadzi sprzedaż towarów, kosmetyków i wyrobów medycznych oraz przeprowadza szkolenia z zakresu zabiegów kosmetycznych.\n",
    "Wnioskodawca zamierza przeprowadzić kampanię reklamową polegającą na przekazywaniu paczek z produktami o wartości do 700 zł brutto. Paczki z produktami mają być opatrzone logo firmy i przekazane do losowo wybranych osób fizycznych, które nie są jego klientami ani kontrahentami. Na zawartość paczki mają składać się pełnowartościowe towary handlowe i dodatkowo gadżety takie jak np. klamra do włosów, pomadka itd. Celem jest promocja firmy, zwiększenie świadomości marki oraz pozyskiwanie przyszłych klientów.\n",
    "Uzupełnienie i doprecyzowanie opisu zdarzenia przyszłego\n",
    "Na pytanie 1: Czy są i będą Państwo na dzień przekazania paczek z produktami zarejestrowani jako czynny podatnik podatku od towarów i usług, odpowiedzieli Państwo: Tak.\n",
    "Na pytanie 2: Czy paczki z produktami będą przez Państwa przekazywane nieodpłatnie, odpowiedzieli Państwo: Tak.\n",
    "Na pytanie 3: Czy w zamian za przekazanie paczek otrzymają Państwo jakiekolwiek wynagrodzenie lub osoby obdarowane zobowiązane będą do wykonania określonych czynności? Jeśli tak, proszę wskazać jakich czynności, odpowiedzieli Państwo: Nie.\n",
    "Na pytanie 4: Czy w odniesieniu do przekazywanych towarów przysługiwało Państwu/będzie Państwu przysługiwało prawo do obniżenia kwoty podatku należnego o kwotę podatku naliczonego z tytułu ich nabycia, importu lub wytworzenia tych towarów lub ich części składowych, odpowiedzieli Państwo: Tak.\n",
    "Na pytanie 5: Czy przekazanie paczek z produktami w ramach kampanii reklamowej nastąpi wyłącznie na cele związane z Państwa działalnością gospodarczą, odpowiedzieli Państwo: Tak.\n",
    "Na pytanie 6: Czy przekazywane produkty będą związane z prowadzoną przez Państwa działalnością gospodarczą? W jaki sposób, odpowiedzieli Państwo: Tak, są to towary handlowe nabywane w celu dalszej odsprzedaży. Dodatkowo paczki będą zawierały gadżety takie jak klamra do włosów, pomadka. Z naszych towarów handlowych i dodatkowych gadżetów będzie przygotowywana paczka PR-owa.\n",
    "Na pytanie 7: Jaką funkcję pełnią przekazywane produkty, np. funkcję promocyjną, informacyjną, itp., odpowiedzieli Państwo: Przekazywane produkty mają pełnić funkcję promocyjną, zwiększyć świadomość marki i pozyskać przyszłych klientów.\n",
    "Na pytanie 8: Czy przekazywane produkty mają wartość konsumpcyjną lub też użytkową dla konsumentów, odpowiedzieli Państwo: Tak.\n",
    "Na pytanie 9: Czy będą Państwo prowadzili ewidencję pozwalającą na ustalenie tożsamości odbiorców, odpowiedzieli Państwo: Nie.\n",
    "Na pytanie 10: Czy na paczki z produktami będą się składały prezenty o małej wartości, tj. przekazywane jednej osobie towary:\n",
    "1) łącznej wartości nieprzekraczającej w roku podatkowym kwoty 100 zł (bez podatku), jeżeli prowadzą Państwo ewidencję pozwalającą na ustalenie tożsamości obdarowanych osób;\n",
    "2) których przekazania nie ujęto w ewidencji, jeżeli jednostkowa cena nabycia towaru (bez podatku), a gdy nie ma ceny nabycia, jednostkowy koszt wytworzenia, określone w momencie przekazywania towaru, nie przekraczają 20 zł,\n",
    "odpowiedzieli Państwo: Nie.\n",
    "Na pytanie 11: Czy produkty przekazywane w paczkach będą stanowiły próbki, przez które należy rozumieć egzemplarz towaru lub jego niewielką ilość, które pozwalają na ocenę cech i właściwości towaru w jego końcowej postaci? Czy ewentualne przekazanie próbki będzie miało na celu promocję tego towaru, odpowiedzieli Państwo: Nie.\n",
    "Na pytanie 12: Czy przekazywana próbka nie będzie służyła zasadniczo zaspokojeniu potrzeb odbiorcy końcowego w zakresie przekazywanego towaru? A jeśli będzie, to czy zaspokojenie potrzeb tego odbiorcy jest nieodłącznym elementem promocji tego towaru i ma skłaniać tego odbiorcę do zakupu promowanego towaru, odpowiedzieli Państwo: Nie będą to próbki.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01af5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text_by_header_regex(text: str) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Split text into chunk using header\n",
    "    \"\"\"\n",
    "    \n",
    "    text_lines = [line.strip() for line in text.splitlines() if line.strip()] \n",
    "    \n",
    "    HEADER_PATTERN = r'([A-ZŚĆŻŹŃŁÓĘĄ][^\\n]*[^\\.\\?!:]\\s*)$'\n",
    "    \n",
    "    chunks_content = []\n",
    "    current_chunk = []\n",
    "    \n",
    "    is_initial_header_gathering = True \n",
    "    \n",
    "    for i, line in enumerate(text_lines):\n",
    "        \n",
    "        is_header = re.search(HEADER_PATTERN, line)\n",
    "        \n",
    "       \n",
    "        if is_initial_header_gathering:\n",
    "            if is_header:\n",
    "                current_chunk.append(line)\n",
    "            else:\n",
    "                is_initial_header_gathering = False\n",
    "                current_chunk.append(line) \n",
    "            \n",
    "            continue \n",
    "\n",
    "        if is_header:\n",
    "            if current_chunk:\n",
    "                chunks_content.append('\\n'.join(current_chunk))\n",
    "                current_chunk = []\n",
    "            \n",
    "            current_chunk.append(line)\n",
    "            \n",
    "        else:\n",
    "            current_chunk.append(line)\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks_content.append('\\n'.join(current_chunk))\n",
    "\n",
    "    documents = [Document(page_content=content.strip()) for content in chunks_content if content.strip()]\n",
    "\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffdb56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = split_text_by_header_regex(tekst)\n",
    "\n",
    "print(f\"Liczba uzyskanych fragmentów (chunks): {len(chunks)}\\n\")\n",
    "\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"--- CHUNK {i+1} (Długość: {len(chunk.page_content)} znaków) ---\")\n",
    "    print(chunk.page_content)\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2a9924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# słowa klucze które wzwraca llm z tekstu\n",
    "slowa_kluczowe_lista = [\n",
    "    \"interpretacja indywidualna\", \"stanowisko nieprawidłowe\", \"wniosek\",\n",
    "    \"opodatkowanie\", \"paczki\", \"kampania reklamowa\", \"przekazywanie\",\n",
    "    \"produkty\", \"promocja\", \"działalność gospodarcza\",\n",
    "    \"towary handlowe\", \"gadżety\", \"funkcja promocyjna\",\n",
    "    \"próbki towaru\", \"promocja towaru\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bdbcc99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Konwersja tekstu do małych liter dla spójności\n",
    "tekst_lower = tekst.lower()\n",
    "\n",
    "# Lista docelowa do przechowywania słowników (obiektów)\n",
    "lista_obiektow_zliczen = []\n",
    "\n",
    "# Zliczanie i tworzenie obiektu\n",
    "for fraza in slowa_kluczowe_lista:\n",
    "    # Zliczanie wystąpień\n",
    "    liczba_wystapien = tekst_lower.count(fraza)\n",
    "\n",
    "    # Tworzenie słownika (obiektu) dla danej frazy\n",
    "    obiekt_zliczenia = {\n",
    "        \"fraza\": fraza,\n",
    "        \"liczba_wystapien\": liczba_wystapien\n",
    "    }\n",
    "\n",
    "    # Dodawanie słownika do listy\n",
    "    lista_obiektow_zliczen.append(obiekt_zliczenia)\n",
    "\n",
    "# Wyświetlenie końcowej listy obiektów\n",
    "import json\n",
    "print(json.dumps(lista_obiektow_zliczen, indent=4, ensure_ascii=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02531f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ngramy\n",
    "tokeny = tekst.lower().split()\n",
    "# tworzenie tabeli tlumaczen\n",
    "znaki_do_usuniecia = string.punctuation \n",
    "tabela_tlumaczen = str.maketrans('', '', znaki_do_usuniecia)\n",
    "\n",
    "tekst_bez_interpunkcji = tekst_lower.translate(tabela_tlumaczen)\n",
    "print(f\"Tokeny: {tokeny}\")\n",
    "\n",
    "n = 2\n",
    "ngramy_nltk = list(ngrams(tokeny, n))\n",
    "\n",
    "print(f\"\\nGenerowane {n}-gramy (trigramy):\")\n",
    "print(ngramy_nltk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f94399",
   "metadata": {},
   "outputs": [],
   "source": [
    "# zliczanie wystapien\n",
    "licznik_bigramow = Counter(ngramy_nltk) \n",
    "\n",
    "bigramy_powyzej_1 = {\n",
    "    para: liczba \n",
    "    for para, liczba in licznik_bigramow.items()\n",
    "    if liczba > 0\n",
    "}\n",
    "\n",
    "print(\"Liczba wystąpień każdej pary w podanej liście:\")\n",
    "for para, liczba in bigramy_powyzej_1.items():\n",
    "    print(f\"{para}: {liczba}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2e137f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_krawedzi = []\n",
    "for (w1, w2), waga in bigramy_powyzej_1.items():\n",
    "    lista_krawedzi.append((w1, w2, {\"weight\": waga}))\n",
    "\n",
    "print(lista_krawedzi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93932ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Tworzenie grafu z listy krawędzi\n",
    "WG = nx.Graph()\n",
    "WG.add_edges_from(lista_krawedzi) # Tutaj graf pobiera obiekty z listy\n",
    "\n",
    "# 2. Odwołanie do obiektów (wag) z grafu\n",
    "# Pobranie wag jako etykiet krawędzi\n",
    "labels = nx.get_edge_attributes(WG, \"weight\")\n",
    "\n",
    "# Ustalenie grubości krawędzi na podstawie wag\n",
    "wagi = [d['weight'] for (u, v, d) in WG.edges(data=True)]\n",
    "grubosc_linii = [waga * 0.4 for waga in wagi]\n",
    "\n",
    "# --- Rysowanie ---\n",
    "pos = nx.spring_layout(WG, seed=42) # Stały układ dla powtarzalności\n",
    "\n",
    "# plt.figure(figsize=(20, 18))\n",
    "plt.figure(figsize=(30, 28))\n",
    "plt.axis('off')\n",
    "\n",
    "# Rysowanie węzłów i etykiet\n",
    "nx.draw_networkx_nodes(WG, pos, node_size=1500, node_color='skyblue', alpha=0.8)\n",
    "nx.draw_networkx_labels(WG, pos, font_size=10, font_family='sans-serif', font_weight='bold')\n",
    "\n",
    "# Rysowanie krawędzi z uwzględnieniem wagi (odwołanie do grubosc_linii)\n",
    "nx.draw_networkx_edges(WG, pos, width=grubosc_linii, edge_color='darkslategray', alpha=0.7)\n",
    "\n",
    "# Rysowanie etykiet krawędzi (odwołanie do labels)\n",
    "nx.draw_networkx_edge_labels(WG, pos, edge_labels=labels, font_color='red', font_size=9)\n",
    "\n",
    "plt.title(\"Graf częstotliwości par słów\", fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c89994c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ustalenie stałej listy części mowy do uwzględnienia\n",
    "FILTROWANE_POS = {\"NOUN\", \"VERB\", \"ADJ\"}\n",
    "OKNO_WSPOLWYSTEPOWANIA = 3 # Okno, w którym słowa są uważane za powiązane\n",
    "\n",
    "# --- ETAP PRZETWARZANIA TEKSTU I TWORZENIA KRAWĘDZI ---\n",
    "\n",
    "# Wczytanie modelu języka polskiego\n",
    "try:\n",
    "    nlp = spacy.load(\"pl_core_news_sm\")\n",
    "except OSError:\n",
    "    print(\"Błąd: Nie znaleziono modelu 'pl_core_news_sm'. Upewnij się, że został pobrany komendą 'python -m spacy download pl_core_news_sm'\")\n",
    "    exit()\n",
    "\n",
    "doc = nlp(tekst)\n",
    "lematy_filtrowane = []\n",
    "\n",
    "# Filtracja i lematyzacja słów\n",
    "for token in doc:\n",
    "    # Kluczowe filtrowanie według części mowy: NOUN, VERB, ADJ\n",
    "    if token.pos_ in FILTROWANE_POS and not token.is_punct and len(token.text) >= 2:\n",
    "        lematy_filtrowane.append(token.lemma_)\n",
    "\n",
    "# Tworzenie krawędzi na podstawie współwystępowania w oknie\n",
    "krawedzie = []\n",
    "for i in range(len(lematy_filtrowane)):\n",
    "    # Sprawdzamy współwystępowanie w obrębie zadanego okna\n",
    "    for j in range(i + 1, min(i + 1 + OKNO_WSPOLWYSTEPOWANIA, len(lematy_filtrowane))):\n",
    "        # Normalizacja krawędzi (mniejsze słowo pierwsze) dla łatwiejszego zliczania\n",
    "        u, v = sorted([lematy_filtrowane[i], lematy_filtrowane[j]])\n",
    "        # Zapobiegamy pętlom (słowo powiązane z samym sobą)\n",
    "        if u != v:\n",
    "            krawedzie.append((u, v))\n",
    "\n",
    "# Zliczanie częstotliwości (wag) krawędzi\n",
    "licznik_krawedzi = Counter(krawedzie)\n",
    "\n",
    "# Tworzenie listy krawędzi w formacie akceptowanym przez networkx (z wagą)\n",
    "# Użyjemy list krotek (węzeł1, węzeł2, słownik_atrybutów)\n",
    "lista_krawedzi = [(u, v, {\"weight\": count}) for (u, v), count in licznik_krawedzi.items() if count > 1] # Filtrujemy rzadkie krawędzie\n",
    "\n",
    "# --- ETAP WIZUALIZACJI GRAFU (Twój kod) ---\n",
    "\n",
    "# 1. Tworzenie grafu z listy krawędzi\n",
    "WG = nx.Graph()\n",
    "WG.add_edges_from(lista_krawedzi) # Tutaj graf pobiera obiekty z listy\n",
    "\n",
    "# 2. Odwołanie do obiektów (wag) z grafu\n",
    "# Pobranie wag jako etykiet krawędzi\n",
    "labels = nx.get_edge_attributes(WG, \"weight\")\n",
    "\n",
    "# Ustalenie grubości krawędzi na podstawie wag\n",
    "wagi = [d['weight'] for (u, v, d) in WG.edges(data=True)]\n",
    "grubosc_linii = [waga * 0.4 for waga in wagi] # Skalowanie jest kluczowe\n",
    "\n",
    "# --- Rysowanie ---\n",
    "pos = nx.spring_layout(WG, seed=42) # Stały układ dla powtarzalności\n",
    "\n",
    "plt.figure(figsize=(30, 28))\n",
    "plt.axis('off')\n",
    "\n",
    "# Rysowanie węzłów i etykiet\n",
    "nx.draw_networkx_nodes(WG, pos, node_size=1500, node_color='skyblue', alpha=0.8)\n",
    "nx.draw_networkx_labels(WG, pos, font_size=10, font_family='sans-serif', font_weight='bold')\n",
    "\n",
    "# Rysowanie krawędzi z uwzględnieniem wagi (odwołanie do grubosc_linii)\n",
    "nx.draw_networkx_edges(WG, pos, width=grubosc_linii, edge_color='darkslategray', alpha=0.7)\n",
    "\n",
    "# Rysowanie etykiet krawędzi (odwołanie do labels)\n",
    "nx.draw_networkx_edge_labels(WG, pos, edge_labels=labels, font_color='red', font_size=9)\n",
    "\n",
    "plt.title(f\"Graf Współwystępowania (Rzeczowniki, Czasowniki, Przymiotniki, okno={OKNO_WSPOLWYSTEPOWANIA})\", fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d02739c",
   "metadata": {},
   "outputs": [],
   "source": [
    "polish_stopwords = set(STOPWORDS)\n",
    "polish_stopwords.update(['jest', 'są', 'się', 'dla', 'aby', 'które', 'jakie', 'w', 'ul', 'o', 'że', 'b', 'z', 'od'])\n",
    "\n",
    "# utworzenie obiektu wordcloud\n",
    "wordcloud = WordCloud(\n",
    "    width=800,\n",
    "    height=400,\n",
    "    background_color='white',\n",
    "    stopwords=polish_stopwords,\n",
    "    min_font_size=10,\n",
    "    font_path='arial.ttf' \n",
    ").generate(tekst.lower())\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Chmura Słów na podstawie tekstu\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1345846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# przetwarzanie tekstu i zliczanie slow wg czesci mowy\n",
    "doc = nlp(tekst)\n",
    "\n",
    "POLSKIE_POS = {\n",
    "    \"ADJ\": \"Przymiotnik\",\n",
    "    \"ADP\": \"Przyimek\",\n",
    "    \"ADV\": \"Przysłówek\",\n",
    "    \"AUX\": \"Czasownik posiłkowy\",\n",
    "    \"CONJ\": \"Spójnik\",\n",
    "    \"CCONJ\": \"Spójnik współrzędny\",\n",
    "    \"DET\": \"Zaimek\",\n",
    "    \"INTJ\": \"Wykrzyknik\",\n",
    "    \"NOUN\": \"Rzeczownik\",\n",
    "    \"NUM\": \"Liczebnik\",\n",
    "    \"PART\": \"Partykuła\",\n",
    "    \"PRON\": \"Zaimek\",\n",
    "    \"PROPN\": \"Nazwa własna\",\n",
    "    \"PUNCT\": \"Interpunkcja\",\n",
    "    \"SCONJ\": \"Spójnik podrzędny\",\n",
    "    \"SYM\": \"Symbol\",\n",
    "    \"VERB\": \"Czasownik\",\n",
    "    \"X\": \"Inne\",\n",
    "    \"SPACE\": \"Spacja\"\n",
    "}\n",
    "\n",
    "pos_data = []\n",
    "\n",
    "for token in doc:\n",
    "    # filtrujemy nieistotne elementy i krotkie slowa\n",
    "    if token.is_stop or token.is_punct or token.is_space or len(token.text) < 3:\n",
    "        continue\n",
    "\n",
    "    angielski_pos = token.pos_\n",
    "    polski_pos = POLSKIE_POS.get(angielski_pos, angielski_pos)\n",
    "    \n",
    "    # Zbieramy czesc mowy + lemat\n",
    "    # pos_data.append((token.pos_, token.lemma_))\n",
    "    pos_data.append((polski_pos, token.lemma_))\n",
    "\n",
    "\n",
    "# Zliczanie częstotliwości występowania dla każdej pary (POS, Lemat)\n",
    "pos_counts = Counter(pos_data)\n",
    "\n",
    "# 4. Przygotowanie danych dla wykresu\n",
    "df = pd.DataFrame([\n",
    "    {\n",
    "        'Część Mowy': pos,\n",
    "        'Lemat': lemat,\n",
    "        'Waga': waga\n",
    "    }\n",
    "    for (pos, lemat), waga in pos_counts.items()\n",
    "])\n",
    "\n",
    "unikalne_pos = df['Część Mowy'].unique()\n",
    "unikalne_pos = unikalne_pos[(unikalne_pos != 'X') & (unikalne_pos != 'NUM')]\n",
    "\n",
    "LIMIT_SLOW = 50 \n",
    "\n",
    "for pos_category in unikalne_pos:\n",
    "    df_filtered = df[df['Część Mowy'] == pos_category].sort_values(by='Waga', ascending=False).head(LIMIT_SLOW)\n",
    "    \n",
    "    if df_filtered.empty:\n",
    "        continue\n",
    "\n",
    "    # Ustawienie MNIEJSZEJ szerokości (np. 8 cali zamiast 10)\n",
    "    # Zmniejszenie współczynnika wysokości (np. z 0.4 do 0.3 cala na słowo)\n",
    "    NOWA_SZEROKOSC = 7\n",
    "    NOWA_WYSOKOSC = len(df_filtered) * 0.3 + 1 \n",
    "    \n",
    "    plt.figure(figsize=(NOWA_SZEROKOSC, NOWA_WYSOKOSC)) \n",
    "    \n",
    "    sns.barplot(\n",
    "        x='Waga', \n",
    "        y='Lemat', \n",
    "        data=df_filtered,\n",
    "        color=sns.color_palette(\"viridis\")[0]\n",
    "    )\n",
    "\n",
    "    plt.title(f'TOP {LIMIT_SLOW} Najczęściej Występujących Słów: {pos_category}')\n",
    "    plt.xlabel('Częstotliwość (Waga)')\n",
    "    plt.ylabel('Słowo (Lemat)')\n",
    "    \n",
    "    plt.xlim(0, max(df_filtered['Waga']) * 1.1)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eutest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
